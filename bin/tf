#!/bin/bash

# Terraform wrapper for use with AWS S3 that ensures a state bucket
# and lock table exist and are tracked in state, then runs the requested
# terraform command.

set -euo pipefail

usage() {
  cat >&2 <<EOM
usage: $(basename "$0") [STACK] [TERRAFORM_ARGUMENTS]

Wrapper for Terraform usage with S3 state backing

STACK: Stack name under stacks/ directory
TERRAFORM_ARGUMENTS: All the usual args like plan, apply, etc

Requires AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY to be set.
EOM
}

check_required_bins() {
    for i in aws jq terraform; do
        which $i > /dev/null || (echo "Missing command: $i" && exit 1)
    done
}

assure_state_bucket() {
    local STATE_BUCKET="$1"
    if aws s3 ls s3://$STATE_BUCKET; then
        echo "Detected bucket $STATE_BUCKET"
    else
        aws s3 mb s3://$STATE_BUCKET
        echo "Created bucket $STATE_BUCKET"
    fi
}

track_state_bucket() {
    local STATE_BUCKET=$1
    # Checking for the state is meh, so just try to import it
    terraform import aws_s3_bucket.tf_state "$STATE_BUCKET" > /dev/null &2>1 || true
    # Now return that state so this explodes if the import failed
    echo "Tracking $STATE_BUCKET in $(terraform state list aws_s3_bucket.tf_state)"
}

assure_lock_table() {
    local LOCK_TABLE="$1"
    if aws dynamodb describe-table --table-name $LOCK_TABLE >/dev/null 2>&1 ; then
        echo "Detected lock table $LOCK_TABLE"
    else
        aws dynamodb create-table 
          --table-name $LOCK_TABLE \
          --attribute-definitions AttributeName=LockID,AttributeType=S \
          --key-schema AttributeName=LockID,KeyType=HASH \
          --sse-specification Enabled=true \
          --provisioned-throughput ReadCapacityUnits=2,WriteCapacityUnits=1

	    aws dynamodb wait table-exists --table-name $LOCK_TABLE
        
        echo "Created lock table $LOCK_TABLE"
    fi
}

track_lock_table() {
    local LOCK_TABLE="$1"
    terraform import aws_dynamodb_table.tf_lock "$LOCK_TABLE" > /dev/null &2>1 || true
    echo "Tracking $LOCK_TABLE in $(terraform state list aws_dynamodb_table.tf_lock)"    
}

terraform_init() {
    local STATE_BUCKET="$1"
    local STACK_NAME="$2"
    terraform init -backend-config="bucket=$STATE_BUCKET" \
      -backend-config="key=states/$STACK_NAME" \
      -backend-config="dynamodb_table=$STACK_NAME"
}

main() {
    check_required_bins

    if [ -z "${1:-}" ]; then
        usage
        exit 1
    fi

    # Get account ID to use to ensure unique bucket names
    ACCOUNT_ID=${ACCOUNT_ID:-$(aws sts get-caller-identity | jq -r .Account)}

    STATE_BASE=${STATE_BASE:-"vn-infra-state"}

    # Make AWS region available as var.region in Terraform
    export TF_region=${AWS_REGION:-"us-east-1"}

    # Set unique state bucket name and expose to Terraform
    # as var.state_bucket
    export TF_state_bucket=$STATE_BASE-$TF_region-$(echo "$STATE_BUCKET:$ACCOUNT_ID" | sha1sum | head -c8)

    STACK=$1

    # Create a name suitable for a DynamoDB table
    STACK_NAME=$(echo $STACK | tr '/' '_')

    # Set lock table name and expose to Terraform as
    # var.lock_table
    export TF_lock_table=$STATE_BASE-$STACK_NAME

    SCRIPT_DIR=$(dirname "$0")
    FULL_STACK_DIR=$(basename $SCRIPT_DIR)/stacks/$STACK

    if [ ! -d $FULL_STACK_DIR ]; then
        echo "FATAL: No stack directory found at $FULL_STACK_DIR"
        exit 1
    fi

    # Enter our actual stack dir
    popd $FULL_STACK_DIR
    # Setup to return to the current dir after exit
    trap popd EXIT

    # Make sure state bucket exists
    assure_state_bucket "$TF_state_bucket"
    
    # Track state bucket only for special "accunt" level stack
    if [ "$STACK" = "account"]; then
        track_state_bucket "$TF_state_bucket"
    fi

    # Make sure lock table exists and is tracked
    assure_lock_table "$TF_lock_table"
    track_lock_table "$TF_lock_table"

    # Initialize Terraform with backend
    terraform_init "$TF_state_bucket" "$STACK_NAME"

    # And now do what was asked
    echo terraform $*
}

main $*
